{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdHh/fQBoigITrNXmzE8rJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advik-7/Automated-workflow-for-BPM-BPO/blob/main/ASR_nd_Diarization_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
        "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n"
      ],
      "metadata": {
        "id": "HllJyjv6brlY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "560ix7YpbZkX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def transcribe_and_diarize(mp3_path, num_speakers=2, model_size=\"base\", output_file=\"transcription.txt\"):\n",
        "    \"\"\"\n",
        "    Transcribes and diarizes an audio file with multiple speakers.\n",
        "\n",
        "    Parameters:\n",
        "        mp3_path (str): Path to the MP3 file.\n",
        "        num_speakers (int): Number of speakers in the audio.\n",
        "        model_size (str): Whisper model size ('tiny', 'base', 'small', 'medium', 'large').\n",
        "        output_file (str): Name of the output file to save the transcription.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved transcription file.\n",
        "    \"\"\"\n",
        "    if not mp3_path.endswith('.wav'):\n",
        "        wav_path = 'temp_audio.wav'\n",
        "        try:\n",
        "            subprocess.call(['ffmpeg', '-i', mp3_path, wav_path, '-y'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            if not os.path.exists(wav_path):\n",
        "                raise FileNotFoundError(\"Conversion to WAV failed. Ensure the input file is valid.\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to convert MP3 to WAV: {e}\")\n",
        "    else:\n",
        "        wav_path = mp3_path\n",
        "\n",
        "    try:\n",
        "        model = whisper.load_model(model_size)\n",
        "        result = model.transcribe(wav_path)\n",
        "        segments = result[\"segments\"]\n",
        "\n",
        "        with contextlib.closing(wave.open(wav_path, 'r')) as f:\n",
        "            frames = f.getnframes()\n",
        "            rate = f.getframerate()\n",
        "            duration = frames / float(rate)\n",
        "\n",
        "        audio = Audio()\n",
        "        embedding_model = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\", device=torch.device(\"cuda\"))\n",
        "        embeddings = np.zeros(shape=(len(segments), 192))\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            start = segment[\"start\"]\n",
        "            end = min(duration, segment[\"end\"])\n",
        "            clip = Segment(start, end)\n",
        "            waveform, sample_rate = audio.crop(wav_path, clip)\n",
        "            embeddings[i] = embedding_model(waveform[None])\n",
        "\n",
        "        embeddings = np.nan_to_num(embeddings)\n",
        "        clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "        labels = clustering.labels_\n",
        "\n",
        "        for i in range(len(segments)):\n",
        "            segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "        def time(secs):\n",
        "            return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            for i, segment in enumerate(segments):\n",
        "                if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "                    f.write(f\"\\n{segment['speaker']} {time(segment['start'])}\\n\")\n",
        "                f.write(segment[\"text\"][1:] + \" \")\n",
        "                f.write(\"\\n\\n\")  # Add extra spacing between speaker segments for better readability.\n",
        "\n",
        "        if wav_path == 'temp_audio.wav':\n",
        "            os.remove(wav_path)\n",
        "\n",
        "        return f\"Transcription saved to {output_file}\"\n",
        "    except Exception as e:\n",
        "        if wav_path == 'temp_audio.wav' and os.path.exists(wav_path):\n",
        "            os.remove(wav_path)\n",
        "        raise RuntimeError(f\"Error during transcription or diarization: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio=\"/content/clipped_audio.mp3\"\n",
        "transcribe_and_diarize(audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "F9w4vJnTbckS",
        "outputId": "037f784b-1bca-439e-c748-514f2958ce3e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  stats = torch.load(path, map_location=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Transcription saved to transcription.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xPxrwtJcEl1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}